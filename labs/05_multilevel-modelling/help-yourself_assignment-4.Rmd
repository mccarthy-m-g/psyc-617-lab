---
title: "Help yourself: Assignment 4"
author:
  - name: "Michael McCarthy"
    url: https://github.com/mccarthy-m-g
    affiliation: "PSYC 617 Lab"
    affiliation_url: https://github.com/mccarthy-m-g/psyc-617-lab
repository_url: https://github.com/mccarthy-m-g/psyc-617-lab
output:
  distill::distill_article:
    toc: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, comment = "#>", collapse = TRUE)
```

## Prerequisites

To access the datasets, help pages, and functions that we will use in this *help yourself*, load the following packages:

```{r prerequisites}
here::i_am("labs/05_multilevel-modelling/help-yourself_assignment-4.Rmd")

library(here)
library(tidyverse)
library(broom.mixed)
library(lme4)
library(parameters)
library(performance)
library(effectsize)
library(emmeans)
library(afex)
library(car)
library(equatiomatic)
```

As well as this Help Yourself's example data:

```{r example-data}
sales <- readr::read_csv(
  here::here("data", "help-yourself_a4.csv"),
  col_types = "fnfnfn"
)
```

## Terminology

This is ridiculous.

```{r}
# Credit to https://twitter.com/ChelseaParlett/status/1458461737431146500?s=20
knitr::include_graphics(here::here("images", "multilevel-models.jpg"))
```

From [Wikipedia](https://en.wikipedia.org/wiki/Multilevel_model): "Multilevel models (also known as hierarchical linear models, linear mixed-effect model, mixed models, nested data models, random coefficient, random-effects models, random parameter models, or split-plot designs) are statistical models of parameters that vary at more than one level."

And if that isn't bad enough:

- **Hierarchical linear models** are a type of linear regression model in which observations fall into hierarchical, or completely nested levels.
- **Hierarchical regression**, on the other hand, is a model-building technique in any regression model (e.g., forward or backward selection).

I kid you not.

Watch out for the terminology, but don't let it get in the way. Pick whichever term you like. I'm partial to linear mixed effect models (or mixed models for short) so I'll be going with that here.

## Linear models

Let's start with a simple linear regression before we dive in. Here the `sales` variable represents the average number of sales a month a salesperson makes in a year. Presumably some positive relationship exists between their years of experience as a salesperson and their sales---the model seems to support this.

```{r}
lm(sales ~ years_experience, data = sales) %>% 
  summary()
```

## Linear mixed models

However, if you inspect the data you will notice that the salespersons work in different car dealerships. Twenty to be exact.

```{r}
sales %>% 
  dplyr::distinct(dealership)
```

It would probably be good to model which dealership a salesperson belongs to, so our model better matches reality. There are likely differences between dealerships that affect a salesperson's sales, and we want to account for that.

We can use the `lmer()` function (`?lme4::lmer`) from the **lme4** package to fit linear mixed effects models to explore these relationships. Note that the lme4 package also has a `glmer()` function for fitting generalized linear mixed effects models, and an `nlmer()` function for fitting nonlinear mixed effects models. You may also be interested in the **glmmTMB** package, which can everything the lme4 package can and more. And if you need even more flexibility then you can use the **brms** package, which takes a Bayesian approach.

The lme4 package has its own formula syntax. It's the same as what you're used to for the most part, but has a bit extra so you can specify random effects. We'll build a few models to show how it works.

<!-- Note: if you like the lme4 formula syntax you can also use it for ANOVA models using the `aov_4()` function instead of the `aov_car()` function in the **afex** package (like we did in PSYC 615). -->

### Null model

Our null model in linear regression would look like this, an intercept only model.

```{r}
sales_null_model <- lm(sales ~ 1, data = sales)

summary(sales_null_model)
```

If we want to take dealership into account, we can build an intercept only linear mixed effects model where the intercept is a random effect. This will give us coefficient estimates for the fixed and random effects.

```{r}
sales_null_mixed_model <- lme4::lmer(sales ~ 1 + (1 | dealership), data = sales)

summary(sales_null_model)
```

We can also compare these models with a likelihood ratio test using the `anova()` function.

```{r}
anova(sales_null_mixed_model, sales_null_model, test = "LRT")
```

### Random intercept model

Now let's add the predictor back. First in the linear model.

```{r}
sales_model_1 <- lm(sales ~ years_experience, data = sales)

summary(sales_model_1)
```

Now in the mixed model.

```{r}
sales_mixed_model_1 <- lme4::lmer(
  sales ~ years_experience + (1 | dealership),
  data = sales
)

summary(sales_mixed_model_1)
```

### Random intercept and slope model

There are two ways to add random slopes to a mixed model. The first is to have an uncorrelated random intercept and slope.

```{r}
# FIXME: Not converging!
sales_mixed_model_2 <- lme4::lmer(
  sales ~ years_experience + (1 | dealership) + (0 + years_experience | dealership),
  data = sales
)

summary(sales_mixed_model_2)
```

The second is to have a correlated random intercept and slope.

```{r}
sales_mixed_model_3 <- lme4::lmer(
  sales ~ years_experience + (1 + years_experience | dealership),
  data = sales
)

summary(sales_mixed_model_3)
```

Including correlations between the random intercept and slope may lead to overly complex models, so model comparisons might help you decide whether or not to include it.

## Convergence issues

The random intercept and slope models above both failed to converge. You should not ignore this. You can however take some steps to decide how to assess and hopefully resolve it. The `?lme4::convergence` help page discusses how you can assess convergence for fitted models. The gold standard approach suggested by the lme4 authors is to use the `allFit()` function (`?lme4::allFit`) to try the fit with all available optimizers. If all optimizers converge to values that are practically equivalent, then we can consider the convergence warnings to be false positives.

```{r}
# Note: if you get an error saying "there is no package called ...", install
# the package(s) and that should resolve the error.
# Note 2: This function has an argument for parallel processing if you find it
# takes too long. This will speed it up.
convergence_assessment <- lme4::allFit(sales_mixed_model_3)

summary(convergence_assessment)
```

Here we see the following.

Under `msgs`:

- 3 optimizers had no warnings or errors (bobyqa, Nelder_Mead, nlminbwrap).
- 3 optimizers failed to converge (nmkbw, optimx.L-BFGS-B, nloptwrap.NLOPT_LN_BOBYQA).
- 1 optimizer had a singular fit (nloptwrap.NLOPT_LN_NELDERMEAD); see the `?lme4::isSingular` help page.

Under the headings for different estimates:

- Values are practically equivalent for all the optimizers, except for the nloptwrap.NLOPT_LN_NELDERMEAD optimizer which had a singular fit.
- When we get a singular fit it suggests our model might need to be simplified (i.e., drop random slopes). It is very common for overfitted mixed models to result in singular fits.
- But we only get singular fit with one optimizer so kind of a mixed message here. Ideally we start with theories and hypotheses before doing statistics so we can take a principled approach to situations like this. 

Under `theta`:

- The theta value for `dealership.years_experience` (i.e., the random slope) is zero for the nloptwrap.NLOPT_LN_NELDERMEAD optimizer which is the likely cause of the singular fit (singularity means that some of the thetas are exactly zero in simple cases).

Making a decision:

- Here I'm going to decide the convergence issues were a false positive, and we don't need to worry about the single case of singularity. This may or may not be the right decision. When you run into issues like this it's good to take some perspective and ask if what you're trying to do statistically is appropriate given the data available to you. Doing further exploratory data analysis might help here, and again, if you go into this with a priori theory and hypotheses as well.
- It might also be beneficial to decide how you will deal with convergence issues before running any statistical tests. Convergence issues are common, and coming in with a plan will reduce the chances you make a bad decision after seeing some results.

Anyways, since we've decided the convergence issues were a false positive, we can either move on with the model as is, or refit with one of the optimizers that converged. Here we'll refit just to demonstrate how to select different optimizers. The `lmerControl()` function (`?lme4::lmerControl`) has a number of arguments that allow you to control how mixed model fitting is done.

```{r}
sales_mixed_model_4 <- lme4::lmer(
  sales ~ years_experience + (1 + years_experience | dealership),
  control = lme4::lmerControl(optimizer = "bobyqa"),
  data = sales
)

summary(sales_mixed_model_4)
```

As with everything else, we should be transparent about what we did here in our statistical report. Even if we don't run into convergence issues, we should still declare the optimization algorithm we chose in our report (and cite its authors!). If you stick with the default optimizer, you can see what that is on the `?lme4::lmerControl` help page.

- Using allFit() with (g)lmer <https://joshua-nugent.github.io/allFit/>
- How scared should we be about convergence warnings in lme4? <https://stats.stackexchange.com/questions/110004/how-scared-should-we-be-about-convergence-warnings-in-lme4>

## Significance tests for fixed effects coefficients

You might have noticed there are no significance tests for the coefficients. There is a very good reason for this: For anything beyond special cases that correspond to classical experimental designs (i.e. balanced designs that are nested, split-plot, randomized block, etc.), the [residual degrees of freedom are unknown](https://bbolker.github.io/mixedmodels-misc/glmmFAQ.html#why-doesnt-lme4-display-denominator-degrees-of-freedomp-values-what-other-options-do-i-have). So the authors of the lme4 package [made a conscious decision](https://stat.ethz.ch/pipermail/r-help/2006-May/094765.html) not to provide any degrees of freedom; as a consequence you do not get a significance test for the coefficients. No stargazing today.

If you *need* a p-value there are a few ways to go about getting one. Every single one of these ways is an ad-hoc solution at best, and you should interpret these p-values with even more caution than usual. Every coefficient comes with a parameter estimate and standard error which you can use to make an informed judgement with or *without* adding p-values into the mix. Anyways, here are the approaches you can take:

- Likelihood ratio tests
- Applying the $z$ distribution to the Wald $t$ values of coefficients ($t$-as-$z$)
- Parametric bootstrapping
- Likelihood profiles
- Kenward-Roger approximation
- Satterthwaite approximation
- Guess the residual degrees of freedom from standard rules for standard designs 

The fitting method you use (maximum likelihood or restricted maximum likelihood) will influence the p-values (REML tends to be better) and determine which approaches are available to you.

I suggest you read the following as a jumping off point and make an informed decision for yourself about whether you need p-values, and if you do, what the best approach is to getting them:

- Evaluating significance in linear mixed-effects models in R <https://link.springer.com/article/10.3758/s13428-016-0809-y>
- GLMM FAQ: Model diagnostics <https://bbolker.github.io/mixedmodels-misc/glmmFAQ.html#model-diagnostics>
- lmerTest Package: Tests in Linear Mixed Effects Models <https://www.jstatsoft.org/article/view/v082i13>

If you decide you need p-values, the `model_parameters()` function (`?parameters::model_parameters.merMod`) from the **parameters** package supports most of the approaches discussed above. The `ci_method` argument can be used to specify the method for computing degrees of freedom for confidence intervals (CI) and the related p-values. The following approaches are available if you are using the lme4 package: “wald”, “ml1”, “betwithin”, “kr”, “satterthwaite”, “kenward”, “boot”, “profile”, “residual”, “normal”. A few more approaches are available if you use the glmmTMB package. See section the function's help page for further details.

```{r}
# t-as-z
parameters::model_parameters(sales_mixed_model_4, ci_method = "wald")

# Satterthwaite approximation
parameters::model_parameters(sales_mixed_model_4, ci_method = "satterthwaite")

# Kenward-Roger approximation
parameters::model_parameters(sales_mixed_model_4, ci_method = "kenward")

# Parametric bootstrapping
# Notes:
# - This is quite slow. You can speed it up with fewer iterations, but you want
# lots of iterations for your final estimates (1000 is a good default).
# - We also get singularity warnings on around 27% of the iterations, and the 
# CIs for the random effects correlation do not look trustworthy. Perhaps another
# reason to rethink including random slopes.
parameters::model_parameters(sales_mixed_model_4, ci_method = "boot", iterations = 1000)

# Likelihood profile
# Notes:
# - This is a little slow but not as much as bootstrapping.
# - Some of the random effects CIs here should also give us a pause.
parameters::model_parameters(sales_mixed_model_4, ci_method = "profile")
```

For likelihood ratio tests they can be performed as usual with the `anova()` function by setting up the model so that the parameter can be isolated/dropped. Likelihood profiles serve the same purpose though so if you want a likelihood based method you may as well do that. This will also keep you away from thinking about doing significance tests for random effects, which may [not be advisable](https://bbolker.github.io/mixedmodels-misc/glmmFAQ.html#testing-significance-of-random-effects).

You should report whatever approach you choose somewhere in your report so the reader knows what you did. It might also be good to see how well the different approaches agree if you didn't make a principled decision (and contingency plan) before looking at the results.

Finally, you if you choose to go with Kenward-Roger or Satterthwaite approximation, you may also be interested in the **lmerTest** package. It's a commonly used package which adds approximated degrees of freedom and p-values to the output of the `summary()` and `anova()` functions of `lmer()` output. See the `?lmerTest::summary.lmerModLmerTest` and `?lmerTest::anova.lmerModLmerTest` help pages.

## Effect size (ICC)

The `icc()` function (`?performance::icc`) from the performance package can be used to calculate the intraclass-correlation coefficient (ICC; sometimes also called variance partition coefficient).

```{r}
performance::icc(sales_mixed_model_4)
```

TODO: Are standardized fixed coefficients okay?

```{r}
effectsize::standardize_parameters(sales_mixed_model_4)
```

## Model comparisons

As demonstrated earlier, we can do a likelihood ratio test using the `anova()` function. Any number of nested models can be included here. Note that the models will be refit using maximum likelihood instead of restricted maximum likelihood for these comparisons.

```{r}
anova(sales_mixed_model_4, sales_mixed_model_2, test = "LRT")
```

And other metrics, same as we did for linear regression.

```{r}
performance::compare_performance(sales_mixed_model_4, sales_mixed_model_2) %>% 
  tibble::as_tibble()
```

## Estimation method

The estimation method used by the `lmer()` function is determined by the `REML` argument. By default this is set to `TRUE`. If you want to use maximum likelihood instead, set it to `FALSE`. As far as which is best, I don't have a good recommendation. You'll probably hear restricted maximum likelihood is better, but Douglas Bates (author of the lme4 package) has described it as statistician's original sin <https://stat.ethz.ch/pipermail/r-sig-mixed-models/2015q3/023743.html>, and has some good discussion here <https://stat.ethz.ch/pipermail/r-sig-mixed-models/2015q3/023750.html> and on page 100 in "lme4: Mixed-effects modeling with R" (linked below).

## Assumptions and Diagnostics

You can use the same diagnostic tools from the performance package you used for linear regression on linear mixed effect models.

```{r}
# I'm dropping the outliers plot here because including it was throwing an error
performance::check_model(
  sales_mixed_model_4,
  check = c("vif", "qq", "normality", "linearity", "ncv", "homogeneity", "reqq", "reqq")
)
```

## Interactions

You specify interactions like usual. Here let's see if the relationship between years of experience and sales depends on the sex of a salesperson, while accounting for the random effect of the dealership they work at.

```{r}
sales_mixed_model_5 <- sales %>% 
  # Centre predictors first, see Tom's lecture notes
  dplyr::mutate(
    dplyr::across(
      years_experience, ~ .x - mean(.x, na.rm = TRUE), .names = "{.col}_centred"
    )
  ) %>% 
  # Random intercept model with an interaction between a level 1 and 2 predictor
  lme4::lmer(
  sales ~ years_experience_centred*female + (1 | dealership),
  data = .
)

summary(sales_mixed_model_5)
```

And you can follow up with the **emmeans** package as usual, with either the `emmeans()` or `emtrends()` functions. This is a bad example since the coefficient for female in the model is the same as pairwise test we get in the follow-up, but you get the point. There is one new thing to note: When constructing the reference grid we can choose how degrees of freedom are approximated (as we did above, but with less options).

```{r}
sales_mixed_emm <- emmeans::emmeans(
  sales_mixed_model_5,
  specs = "female",
  lmer.df = "satterthwaite" # or "kenward", or "asymptotic" (t-as-z)
)

emmeans::contrast(sales_mixed_emm, method = "pairwise")
```

You can read the sophisticated models in emmeans vignette for more details <https://cran.r-project.org/web/packages/emmeans/vignettes/sophisticated.html>. And more examples can be found here <https://rpubs.com/palday/mixed-interactions>.

You might also want to read Jason Newsom's lecture notes on centring in mixed models <http://web.pdx.edu/~newsomj/mlrclass/ho_centering.pdf>. Centring is more complicated in linear mixed models than in linear regression, and there's some good discussion on the topic there. It's still important to have good reasons for centring, rather than doing it as a ritual, but there tend to be more good reasons to centre in mixed models. This discussion on centring is also thoughtful <https://centerstat.org/centering/>.

Finally, just a note: You cannot do followups using random effects as a grouping variable with the emmeans package---you can only follow up fixed effects. See the discussion here <https://github.com/rvlenth/emmeans/issues/118>. If you have a discrete predictor you could treat it as both a fixed and random effect, which *may* be appropriate, and will allow followups with the emmeans package. See this blog <https://www.muscardinus.be/2017/08/fixed-and-random/>.

## Repeated measures ANOVA comparison

ANOVA models with repeated measures can be thought of as special cases of linear mixed models. Again, we'll use the Help Yourself data and example from PSYC 615 to demonstrate.

```{r}
# Repeated measures ANOVA
scores <- readr::read_csv(
  here::here("data", "help-yourself_psyc-615_a6.csv"),
  col_types = "fffd"
)
```

Here is a two-way repeated measures ANOVA (i.e., higher order).

```{r}
# Higher order (both factors are repeated)
afex::aov_car(
  score ~ task * level + Error(id/(task*level)),
  type = 3,
  data = scores
) %>% 
  summary()
```

Now as a linear mixed model. We can see a lot of agreement between the two models, except for the sum of squares for the main effects. If we use one of the degrees of freedom approximations we can also see agreement between those and the significance tests in both models. This goes back to the point above that the degrees of freedom for classical experimental designs can likely be safely approximated.  

```{r}
scores_mixed_model <- lme4::lmer(
  score ~ task * level + (1 | id) + (1 | task:id) + (1 | level:id),
  contrasts = list(task = contr.sum, level = contr.sum),
  data = scores
)

anova(scores_mixed_model)
car::Anova(scores_mixed_model, type = 3, test = "F")
```

If we drop some of the random effects we get the same sum of squares as in the ANOVA model. I'm not actually sure why (maybe a Type 3 sum of squares thing?), but I thought I'd point it out. It messes everything else up though. Anyways, point being, ANOVAs that include repeated measures can be thought of as special cases of mixed models.

```{r}
lme4::lmer(
  score ~ task * level + (1 | id),
  contrasts = list(task = contr.sum, level = contr.sum),
  data = scores
) %>% 
  anova()
```

Michael Clark explores these ideas more here <https://m-clark.github.io/docs/mixedModels/anovamixed.html>, and Daniel Wollschlaeger has lme4 code for a number of classical ANOVA equivalent designs here <http://www.dwoll.de/rexrepos/posts/anovaMixed.html>.

## Tidy methods

We use the **broom.mixed** package instead of the **broom** package to use the `tidy()`, `augment()`, and `glance()` functions (`?broom.mixed::tidy.merMod`) on lme4 models. There are a few extra arguments for these functions specific to mixed models; you can read more on the help page. Otherwise they work the same as usal.

```{r}
broom.mixed::tidy(sales_mixed_model_5)
broom.mixed::augment(sales_mixed_model_5)
broom.mixed::glance(sales_mixed_model_5)
```

## Equations

It can be helpful to write the equations down for a mixed model to make sure you are misspecifying anything. The `extract_eq()` function (`?equatiomatic::extract_eq`) from the **equatiomatic** package can do this for you. The authors of the package aim to support every model that is supported by the broom package in the future. Currently it supports models we use in this course such as `lm()`, `glm()`, and `lmer()` models. Mixed models use the notation from [Gelman and Hill](http://www.stat.columbia.edu/~gelman/arm/), so this will probably look unfamiliar to you.

```{r}
# This returns the math code you would put in the body of your R Markdown
# document
equatiomatic::extract_eq(sales_null_mixed_model)
```

$$
\begin{aligned}
  \operatorname{sales}_{i}  &\sim N \left(\alpha_{j[i]}, \sigma^2 \right) \\
    \alpha_{j}  &\sim N \left(\mu_{\alpha_{j}}, \sigma^2_{\alpha_{j}} \right)
    \text{, for dealership j = 1,} \dots \text{,J}
\end{aligned}
$$

You can also use the `tex_preview()` function (`?texPreview::tex_preview`) from the **texPreview** package to preview equations in the RStudio viewer. This requires some additional setup, described here <https://github.com/yonicd/texPreview#functionality>.

NOTE: FOR THE ASSIGNMENT I WILL NOT ACCEPT THIS FORM OF NOTATION; USE THE NOTATION FROM TOM'S LECTURE.

## Learning more

### General

- Learning Multilevel Analysis Using Visualisation of the predicted values <https://psyarxiv.com/7nzx2/>
- Fitting Linear Mixed-Effects Models Using lme4 <https://cran.r-project.org/web/packages/lme4/vignettes/lmer.pdf>
- Mixed Models with R: Getting started with random effects <https://m-clark.github.io/mixed-models-with-R/>
- Thinking About Mixed Models <https://m-clark.github.io/docs/mixedModels/mixedModels.html>
- GLMM FAQ (lots of valuable info here; you should read through the whole thing at least once) <https://bbolker.github.io/mixedmodels-misc/glmmFAQ.html>
- An Introduction to Mixed Models for Experimental Psychology <http://singmann.org/download/publications/singmann_kellen-introduction-mixed-models.pdf>
- How to analyze linguistic change using mixed models, Growth Curve Analysis and Generalized Additive Modeling (this is a really great paper) <https://academic.oup.com/jole/article/1/1/7/2281883#82980835>

### ANOVA comparison

- ANOVA and mixed models <https://stat.ethz.ch/~meier/teaching/anova/random-and-mixed-effects-models.html>

### Optimizers

- On Best Practice Optimization Methods in R (this paper has some good references about optimizers; it probably isn't relevant to you) <https://www.jstatsoft.org/article/view/v060i02>

### Ordinal mixed models

The ordinal package has you covered. Go back to the Help Yourself for assignment 1 for links.

## Cross Validated

- How to choose random- and fixed-effects structure in linear mixed models? <https://stats.stackexchange.com/q/130714/337979>

### R

- R's lmer cheat sheet <https://stats.stackexchange.com/questions/13166/rs-lmer-cheat-sheet>
- lme4: Mixed-effects modeling with R <https://stat.ethz.ch/~maechler/MEMo-pages/lMMwR.pdf>

### R packages

- equatiomatic <https://datalorax.github.io/equatiomatic/>
